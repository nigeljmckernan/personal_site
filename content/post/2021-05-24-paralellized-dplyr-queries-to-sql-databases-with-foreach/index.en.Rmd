---
title: Parallelized `dplyr` Queries to SQL Databases with `foreach`
author: Nigel McKernan
date: '2021-05-24'
slug: paralellized-dplyr-queries-to-sql-databases-with-foreach
categories:
  - R
tags:
  - dplyr
  - dbplyr
  - SQL
  - foreach
subtitle: ''
summary: ''
authors: []
lastmod: '2021-05-24T22:38:40-04:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---


In my humble opinion, I find `dplyr` to be a genius bit of engineering with how it abstracts *and* simplifies what you _actually_ want to do with potential `SQL` code.

However, I have not seen much coverage on how to take advantage of parallelization of `dplyr` queries with something like `foreach`.

Usually, a post will provide a solution using `parallel::mclappply()` or some other parallelized `apply()`-type function, like [this post](https://stackoverflow.com/a/45442450/12375487) on StackOverflow.

If you're more partial to the way a parallelized `for`-loop might syntactically accomplish the same thing, then I have a solution via `foreach` that allows you to write parallelized `dplyr` queries off to your SQL databases.

**Note**: There is the `multidplyr` package, however that only deals with local in-memory dataframes, and *not* remote databases.

Let's see how we might accomplish this.

For my example dataset, I'm going to use the `nycflights13` package that contains all flights arriving and departing NYC in 2013. It's a great mid-size dataset to experiment with.

Let's get our packages loaded and our databases set up.

For this example I'm going to spin up an in-memory SQLite database.

```{r setup, message=FALSE}
library(DBI)
library(nycflights13)
library(tidyverse)
library(RSQLite)
library(doParallel)

Remote_DB <- dbConnect(SQLite(), ":memory:")

flights %>% 
  
  dbWriteTable(
    conn = Remote_DB,
    name = "Flights",
    value = .
  )

Flights_Remote <- tbl(Remote_DB, "Flights")

Flights_Remote
```

Now that we've set up our database connection, let's setup our `foreach` back-end.

In case you're not aware, in order for `foreach()` to work in a parallelized fashion and *not* via serial operations, `foreach` needs a *back-end* to be registered.


A `foreach` back-end is basically a way to declare to `foreach` *how* to distribute the workload across the worker nodes, whether they be the (probably) many cores in your computer's CPU, different computers on the same network, or more remote cloud-based machines.

In this example, I'm going to use the `doParallel` back-end to carry this out, which will just be spread out across all but one of my computer's CPU cores.

```{r foreach setup}
cl <- makeCluster(detectCores() - 1L)

registerDoParallel(cl)
```

Now that we've registered our back-end with `foreach` let's determine the query we would like to be parallelized.

I'm going to arbitrarily rank the top 5 longest average arrival delays by plane per origin. 

The parallelization will be taken place over the "per origin" part, so let's just start with one origin location for now.

I realise that this is not the most helpful of examples for parallelization, as window functions in SQL can easily handle things like this (ranking, etc.) across different variables and groups.

I'll present a more personal example that I use quite frequently for my job in retail later in this post.

```{r before parallel}

Main_Query <- Flights_Remote %>%
  
  filter(origin == "LGA") %>%
  
  group_by(
    origin,
    tailnum
  ) %>%
  
  summarise(Mean_Arrival = mean(arr_delay, na.rm = TRUE)) %>%
  
  arrange(desc(Mean_Arrival)) %>%
  
  mutate(Rank = row_number()) %>%
  
  filter(Rank <= 5) %>%
  
  ungroup() %>%
  
  collect()

Main_Query
```

Now that we've done that for one origin, let's instead provide a vector of origins to iterate over for our next example.

```{r parallel example}

Origins <- c("LGA", "EWR", "MCO", "ORD", "IAD")

foreach(i = seq_along(Origins),
        .packages = c("dplyr", "DBI", "RSQLite", "nycflights13"),
        .combine = "rbind") %dopar% {
          
        Remote_DB <- dbConnect(SQLite(), ":memory:") # Each worker must make a connection to our in-memory database
        
        dbWriteTable( # If this were on an on-disk database, this dbWriteTable() call wouldn't be necessary
          conn = Remote_DB,
          name = "Flights",
          value = flights
        )
        
        Flights_Remote <- tbl(Remote_DB, "Flights") # Connecting to the Flights table we sent to the database earlier
        
        Flights_Remote %>%
  
          filter(origin == local(Origins[i])) %>%
  
          group_by(
            origin,
            tailnum
            ) %>%
  
          summarise(Mean_Arrival = mean(arr_delay, na.rm = TRUE)) %>%
  
          arrange(desc(Mean_Arrival)) %>%
  
          mutate(Rank = row_number()) %>%
  
          filter(Rank <= 5) %>%
  
          ungroup() %>%
  
          collect() -> Results # Let's save the results in a temporary variable
        
        dbDisconnect(Remote_DB) # Don't forget that each worker must disconnect before closing
        
        Results # foreach pulls the last object determined in the loop to bring out into your main R session
          
        }

```
Like I said previously, I know this is not the best example; a parallelized loop is *not* needed for this, as you can see here:

```{r}

Origins <- c("LGA", "EWR", "MCO", "ORD", "IAD")

Flights_Remote %>%
  
  filter(origin %in% Origins) %>%
  
  group_by(
    origin,
    tailnum
  ) %>%
  
  summarise(Mean_Arrival = mean(arr_delay, na.rm = TRUE)) %>%
  
  arrange(desc(Mean_Arrival)) %>%
  
  mutate(Rank = row_number()) %>%
  
  filter(Rank <= 5) %>%
  
  ungroup()

```

Thanks to Window Functions in SQL, a parallelized loop is not necessary.

Instead I'll bring up an example that might be a bit more relevant to me:

I currently work as a CRM analyst for a Canadian fashion retailer based out of Montr√©al. One of my many duties is to determine the value that a particular store's customers might be worth in our overall market.

Put simply: for a certain store's customers, how much do they spend in our other locations.

The catch is, I usually need to do this for our entire collection of currently-open stores.

I do not know of a way currently to do this *without* implementing a for-loop or `apply()`-style of execution where a list of stores is iterated over to determine its customers and their behaviour elsewhere.


The steps to do this then are:

  1. Determine the window of time
      * Usually this is some 12-month window
  2. Determine the list of customers that shop at the given store in the aforementioned time window
  3. Take that list of customers and aggregate their total sales by store.
  
Again, this is fairly straightforward to do with one or maybe a handful of stores manually.

When it's in the hundreds of stores, it becomes quite tedious and slow quite fast.

So, let's:

  1. Take a window from 12-months prior to today, to today
  2. Pull a **distinct** list of customers for a given store
  3. Use that list to determine what they spend in our other stores
  
The following code chunk will not be evaluated, as I do not have simulated/fake data prepared to demonstrate, and I of course can't display my employer's confidential data.

I'm going to write this in sort of "pseudo-code".

```{r personal example, eval=FALSE}

foreach(i = seq_along(Some_Vector_of_Stores),
        .packages = c("DBI","dplyr","RSQLite"),
        .combine = "rbind") %dopar% {
          
          Database <- dbConnect(SQLite(), "Path/To/SQLite/Database.sqlite") # Connect to our database
          
          Transactions <- tbl(Database, "Transactions") # Connect to the needed table
          
          Transactions %>%
            
            filter(
              Transaction_Date %>% between("Beginning_Date", "End_Date"),
              Store_no == local(Some_Vector_of_Stores[i])
              ) %>%
            
            distinct(
              Customer_ID
              Host_Store == Store_no
              ) %>%
            
            inner_join(
              Transactions %>%
                
                filter(Transaction_Date %>% between("Beginning_Date", "End_Date")),
              
              by = "Customer_ID"
            ) %>%
            
            group_by(
              Host_Store,
              Store_no
            ) %>%
            
            summarise(
              Customers = n_distinct(Customer_ID),
              Spend = sum(Sales)
            ) %>%
            
            ungroup() %>%
            
            collect() -> Results
          
          dbDisconnect(Database)
          
          Results
        }
```


These examples don't strictly *need* to be run in parallel; as long as the syntax is correct, sometimes a serial method is the only way possible.

I more did this example to demonstrate how `dbplyr` queries can be parallelized via the syntax of a `for` loop, specifically. 

I often prefer loops in R compared to either `purrr`'s `map()` family of functions, or their base-R equivalents with the `apply` family of functions.
